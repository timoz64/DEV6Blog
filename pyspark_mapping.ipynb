{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1t-nsOD2vhFtIrdmrXJwf43FCuNp2I9ai",
      "authorship_tag": "ABX9TyMAqiPbHD6fk4kXoHd6FIMP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/timoz64/DEV6Blog/blob/master/pyspark_mapping.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hxEGVEy1ygLl",
        "outputId": "7ffde4eb-ce36-430e-e41f-3e08acf8917c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.3)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ],
      "source": [
        "%pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m0SlxLRw0GtZ",
        "outputId": "d802db5b-f798-4c4f-c8f7-a22f04106501"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark=SparkSession.builder.appName('practice').getOrCreate()"
      ],
      "metadata": {
        "id": "8BT3Eoo3z0rz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col,lit,create_map\n",
        "\n",
        "df = spark.createDataFrame([(\"Alice\", 2), (\"Bob\", 5)], (\"name\", \"age\"))\n",
        "df.select(create_map('name', 'age').alias(\"map\")).collect()\n",
        "\n",
        "df.select(create_map([df.name, df.age]).alias(\"map\")).collect()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "--5jXynoyjSJ",
        "outputId": "266b1c7e-eb23-482c-d94b-0abb63e4079b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(map={'Alice': 2}), Row(map={'Bob': 5})]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "employee_df = spark.createDataFrame(employee_data, [\"ID\", \"Name\", \"Age\", \"Salary\"])\n",
        "filtered_df = employee_df.filter(employee_df.Salary > 5000)\n",
        "filtered_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TMQa2EiX3SrL",
        "outputId": "120289c0-e596-431f-a14b-402b022d3d98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----+---+------+\n",
            "| ID| Name|Age|Salary|\n",
            "+---+-----+---+------+\n",
            "|  2|Smith| 30|  6000|\n",
            "|  4|Henry| 40|  7000|\n",
            "+---+-----+---+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mapped_rdd = employee_rdd.map(lambda x: (x[0], x[1].upper(), x[2], x[3]))\n",
        "print(mapped_rdd.collect())  # Output: [('1', 'JOHN', 28, 5000), ('2', 'SMITH', 30, 6000), ...]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p_aW2UYs3wCI",
        "outputId": "97a7d31d-7688-40fb-d701-8b0ae0132691"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('1', 'JOHN', 28, 5000), ('2', 'SMITH', 30, 6000), ('3', 'ADAM', 35, 4000), ('4', 'HENRY', 40, 7000)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df1 = spark.createDataFrame(employee_data, [\"ID\", \"Name\", \"Age\", \"Salary\"])\n",
        "# df2 = spark.createDataFrame(employee_data, [\"ID\", \"Name\", \"Age\", \"Salary\"])\n",
        "df1.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ffe4jMAj5i0M",
        "outputId": "af32563f-ab16-4623-8362-f01c086afdd0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----+---+------+\n",
            "| ID| Name|Age|Salary|\n",
            "+---+-----+---+------+\n",
            "|  1| John| 28|  5000|\n",
            "|  2|Smith| 30|  6000|\n",
            "|  3| Adam| 35|  4000|\n",
            "|  4|Henry| 40|  7000|\n",
            "+---+-----+---+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: create dataframe df2, that has only one column \"ID\" with values 2 and 3\n",
        "\n",
        "df2 = spark.createDataFrame([(2,), (3,)], [\"ID\"])\n",
        "df2.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nZRu2sBJ5wJ4",
        "outputId": "119ecb55-88ac-4d60-a599-c930262d1fee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+\n",
            "| ID|\n",
            "+---+\n",
            "|  2|\n",
            "|  3|\n",
            "+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# add new colum to df1 called new_column that has value 1 for all rows where column ID value equals with column ID value in df2\n",
        "df1 = df1.join(df2, df1.ID == df2.ID, \"left\").withColumn(\"new_column\", lit(1))\n",
        "df1.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YuAuKYyn6qs9",
        "outputId": "cb036a61-6529-4fba-b869-0d1bdbd95f55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----+---+------+----+----------+\n",
            "| ID| Name|Age|Salary|  ID|new_column|\n",
            "+---+-----+---+------+----+----------+\n",
            "|  1| John| 28|  5000|NULL|         1|\n",
            "|  2|Smith| 30|  6000|   2|         1|\n",
            "|  3| Adam| 35|  4000|   3|         1|\n",
            "|  4|Henry| 40|  7000|NULL|         1|\n",
            "+---+-----+---+------+----+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import when, col\n",
        "\n",
        "# Create the Spark session\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "# Original dataframe\n",
        "data = [(1, \"John\", 28, 5000),\n",
        "        (2, \"Smith\", 30, 6000),\n",
        "        (3, \"Adam\", 35, 4000),\n",
        "        (4, \"Henry\", 40, 7000)]\n",
        "columns = [\"ID\", \"Name\", \"Age\", \"Salary\"]\n",
        "\n",
        "df = spark.createDataFrame(data, columns)\n",
        "\n",
        "# Second dataframe\n",
        "data2 = [(2,), (3,)]\n",
        "columns2 = [\"ID\"]\n",
        "\n",
        "df2 = spark.createDataFrame(data2, columns2)\n",
        "\n",
        "# Perform a left join to check for matching IDs\n",
        "joined_df = df.join(df2, on=\"ID\", how=\"left\")\n",
        "\n",
        "# Add the new column with the required logic\n",
        "result_df = joined_df.withColumn(\n",
        "    \"new_column\",\n",
        "    when(col(\"ID\").isin(df2.select(\"ID\").rdd.flatMap(lambda x: x).collect()), 1).otherwise(\"N/A\")\n",
        ")\n",
        "\n",
        "# Show the result\n",
        "result_df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AvtqrcuI7Myp",
        "outputId": "f7d87c2c-f7e1-4af3-9c05-ef7dbdb16a0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----+---+------+----------+\n",
            "| ID| Name|Age|Salary|new_column|\n",
            "+---+-----+---+------+----------+\n",
            "|  1| John| 28|  5000|       N/A|\n",
            "|  2|Smith| 30|  6000|         1|\n",
            "|  3| Adam| 35|  4000|         1|\n",
            "|  4|Henry| 40|  7000|       N/A|\n",
            "+---+-----+---+------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import StringType\n",
        "\n",
        "# Define the mapping as a dictionary\n",
        "mapping_dict = {2: \"1\", 3: \"1\"}\n",
        "\n",
        "# Define a UDF for mapping\n",
        "def map_function(key):\n",
        "    return mapping_dict.get(key, \"N/A\")\n",
        "\n",
        "map_udf = udf(map_function, StringType())\n",
        "\n",
        "# Add a new column using the UDF\n",
        "df_with_mapping = df.withColumn(\"new_column\", map_udf(col(\"ID\")))\n",
        "\n",
        "df_with_mapping.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iKdnfWZ6_PmT",
        "outputId": "6f472e93-4b3a-4039-c781-802a97f5374b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----+---+------+----------+\n",
            "| ID| Name|Age|Salary|new_column|\n",
            "+---+-----+---+------+----------+\n",
            "|  1| John| 28|  5000|       N/A|\n",
            "|  2|Smith| 30|  6000|         1|\n",
            "|  3| Adam| 35|  4000|         1|\n",
            "|  4|Henry| 40|  7000|       N/A|\n",
            "+---+-----+---+------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import lit, when\n",
        "\n",
        "# Create a mapping dataframe\n",
        "mapping_df = spark.createDataFrame([(2, 1), (3, 1)], [\"ID\", \"mapped_value\"])\n",
        "\n",
        "# Perform a left join\n",
        "joined_df = df.join(mapping_df, on=\"ID\", how=\"left\")\n",
        "\n",
        "# Add a new column with the mapped values or \"N/A\" if no match\n",
        "result_df = joined_df.withColumn(\n",
        "    \"new_column\",\n",
        "    when(col(\"mapped_value\").isNotNull(), col(\"mapped_value\")).otherwise(\"N/A\")\n",
        ").drop(\"mapped_value\")\n",
        "\n",
        "result_df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1lqxgiJdBJfB",
        "outputId": "80d1bd19-588b-48e5-b8be-0b406d791325"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----+---+------+----------+\n",
            "| ID| Name|Age|Salary|new_column|\n",
            "+---+-----+---+------+----------+\n",
            "|  1| John| 28|  5000|       N/A|\n",
            "|  2|Smith| 30|  6000|         1|\n",
            "|  3| Adam| 35|  4000|         1|\n",
            "|  4|Henry| 40|  7000|       N/A|\n",
            "+---+-----+---+------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.types import *\n",
        "\n",
        "# Create broadcast dictionary\n",
        "mapping_dict = {2: \"1\", 3: \"1\"}\n",
        "broadcast_mapping = spark.sparkContext.broadcast(mapping_dict)\n",
        "\n",
        "# Add a column using a lambda function\n",
        "df_with_mapping = df.withColumn(\n",
        "    \"new_column\",\n",
        "    F.expr(f\"CASE WHEN ID IN ({','.join(map(str, broadcast_mapping.value.keys()))}) THEN 1 ELSE 'N/A' END\")\n",
        ")\n",
        "\n",
        "df_with_mapping.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BHgLdwdgBNYX",
        "outputId": "9342533e-389d-444c-8c2a-8d79e3a838fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----+---+------+----------+\n",
            "| ID| Name|Age|Salary|new_column|\n",
            "+---+-----+---+------+----------+\n",
            "|  1| John| 28|  5000|       N/A|\n",
            "|  2|Smith| 30|  6000|         1|\n",
            "|  3| Adam| 35|  4000|         1|\n",
            "|  4|Henry| 40|  7000|       N/A|\n",
            "+---+-----+---+------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# dataframe to dictionary\n",
        "# Create a mapping dataframe\n",
        "mapping_df = spark.createDataFrame([(2, \"1\"), (3, \"1\")], [\"ID\", \"mapped_value\"])\n",
        "\n",
        "# Convert the mapping DataFrame to a dictionary\n",
        "mapping_dict = dict(mapping_df.rdd.map(lambda row: (row[\"ID\"], row[\"mapped_value\"])).collect())\n",
        "\n",
        "print(mapping_dict)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wD1rtuusBqC_",
        "outputId": "cd4dbccc-4c7e-4e2d-d99f-f4b89ba931c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{2: '1', 3: '1'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XQ990jSsB0mq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}